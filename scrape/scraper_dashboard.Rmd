---
title: "Dashboard for BirdLife LitScan scraper"
author: "Bill Oxbury"
date: "`r Sys.Date()`"
output: html_document
---

_This file is updated automatically as part of the scraper processing._


```{r set-up-SQL-version, include=FALSE, warning=FALSE}

# run with:
#
# R -e "Sys.setenv(RSTUDIO_PANDOC='/Applications/RStudio.app/Contents/MacOS/quarto/bin/tools');   rmarkdown::render('scraper_dashboard.Rmd', rmarkdown::html_document(toc = TRUE))"


library(stringr, warn.conflicts=FALSE)
library(dplyr, warn.conflicts=FALSE)
library(dbplyr, warn.conflicts=FALSE)
library(purrr, warn.conflicts=FALSE)
library(lubridate, warn.conflicts=FALSE)
library(ggplot2, warn.conflicts=FALSE)

MAX_DAYS <- 2200
start_date <- today() - MAX_DAYS

source('/Volumes/blitshare/pg/param.txt')

conn <- DBI::dbConnect(
  RPostgres::Postgres(),
  bigint = 'integer',  
  host = PGHOST,
  port = 5432,
  user = PGUSER,
  password = PGPASSWORD,
  dbname = PGDATABASE)


df <- tbl(conn, 'links')
df_tx <- df %>% filter(GOTTEXT == 1 & 
                         BADLINK == 0 & 
                         score > -20.0 &
                         (is.na(date) | date > start_date) )
```


### First statistics

The
<a href="https://blitscanapp.azurewebsites.net/">BlitScan app</a> is built over a data set of journal articles/research publications:

```{r nr-records, echo=FALSE, warning=FALSE}

nr <- df_tx %>% 
  summarise(count = n()) %>%
  select(count) %>%
  collect() %>% 
  as.numeric() 

cat("Number of articles:", format(nr, big.mark = ','), '\n')
```

```{r nr-species, echo=FALSE, warning=FALSE}

species <- df_tx %>% 
  select(species) %>%
  filter(!is.na(species)) %>%
  collect() 

species_nonunique <- species$species %>%
  str_split('\\|') %>%
  unlist() %>%
  as.integer()

species_unique <-  species_nonunique %>%
  unique() 

cat("Number of species:", format( length(species_unique), big.mark = ','), '\n')
```

```{r nr-journals-SQL, echo=FALSE, warning=FALSE}

df_dois <- tbl(conn, 'dois')

title_dois <- inner_join(df_dois, df_tx, by = c('doi','doi')) %>%
  select(Journal = container.title) %>%
  filter(!is.na(Journal)) %>%
  collect()
title_oai <- tbl(conn, 'oai') %>%
  select('Journal') %>%
  collect()
  
title_list <- rbind(title_dois, 
                    title_oai) %>%
  distinct()
nt <- nrow(title_list) %>%
  format(big.mark = ',')

cat("Number of journals/proceedings:", nt, '\n')
```

```{r nr-publishers, echo=FALSE, warning=FALSE}


publishers <- inner_join(df_dois, df_tx, by = c('doi','doi')) %>%
  select(publisher) %>%
  distinct() %>%
  collect()

np <- nrow(publishers) %>%
  format(big.mark = ',')

cat("Number of publishers:", np, '\n')
```

More detail on journals and publishers is given below. A full list of journals/proceedings the database draws from is given in the appendix.


### Red-list coverage

The counts in the table below are the numbers of species (normalised to _SISRecID_ identifier and classified by red-list status) referenced in the database, versus those occuring in the <a href="http://datazone.birdlife.org/home">BirdLife DataZone</a>.

```{r red-list, echo=FALSE, warning=FALSE}

df_redlist <- tbl(conn, 'species') %>%
  select(SISRecID, status) %>%
  collect()

# status counts
df_status <- df_redlist %>%
  filter(!is.na(status)) %>%
  group_by(status) %>%
  summarise(OutOf = n())

# status counts for the blitscan list
df_tx_species <- tibble(SISRecID = species_unique)
df_blit <- inner_join(df_tx_species, df_redlist, 
                      by = 'SISRecID') %>%
  filter(!is.na(status)) %>%
  group_by(status) %>%
  summarise(Count = n())

# join the two
df_out <- full_join(df_blit, df_status, by = 'status') %>%
  mutate(Percent = round( 100*Count/OutOf, 1)) %>%
  rename(Status = status) %>%
  arrange(desc(OutOf))

df_out %>%
  knitr::kable(format = "html", 
               format.args = list(big.mark = ','),
               table.attr = "style='width:50%;'")
```
<p>
(Note that the numbers in the _Count_ column are underestimates and may not add up to the species count given above - this is because red-list status cannot always be ascertained. The numbers in the _OutOf_ column are estimated by a crawl of DataZone pages.)

<p>
How many journal articles are we seeing per-species? 

```{r article-counts, fig.height=3, fig.width=6, echo=FALSE, warning=FALSE}

article_counts <- tibble(SISRecID = species_nonunique) %>%
  group_by(SISRecID) %>%
  summarise(count = n())

df_blit <- inner_join(article_counts, df_redlist, by = 'SISRecID') %>%
  filter(!is.na(status))

df_blit %>%
  group_by(status) %>%
  ggplot(aes(x = reorder(status, -count, min), y = count)) +
  geom_boxplot(colour = "blue",
               fill = "lightblue") +
  scale_y_continuous(trans='log10') +
  labs(y="Article count", x="Status", 
       subtitle="")

```

### Sources

The search process to find web content follows four main steps to identify articles, followed by text retrieval from those articles. These are:

1. Bing Custom Search against targeted domains 
2. Scanning of <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">Open Archives Initiative</a> API to get titles/abstracts from relevant contributors
3. Query of preprint archives against genus (Latin) names in vulnerable categories
4. Scanning of content lists for targeted journals.

#### Step 1: <a href="https://docs.microsoft.com/en-us/bing/search-apis/bing-custom-search/overview">Custom Search</a>

This first step uses the full power of customised web search - previously using Google, now using Microsoft Bing. It gives us a wide reach, while the journal scans in the later steps ensure greater completeness.

The process manages the cost of search by sampling search terms at each run, with the aim of achieving repeated coverage of all search terms over time. Custom search runs queries against a set of targeted domains (which will be listed below after describing steps 2,3,4). Each query uses a random search term taken from a <a href="https://github.com/billoxbury/blitscan/blob/main/data/searchterms_general.txt">list of generic scientific/common bird names</a> (while checking that the search hasn't been used too recently).

URLs returned from this process are stored and mined individually *after* all of steps 1-4 have completed. 

#### Step 2: <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">Open Archives Initiative (OAI)</a>

OAI is a centralised service that collects research metadata, including titles/abstracts, from institutions and publishers around the world. On the one hand, it protects these providers from cyber threats if they permit web crawling directly to their servers; on the other hand, it greatly simplifies the process for data consumers in providing a single format across a very wide community of providers.

As of June 2022 there are 5,616 listed providers to OAI. Most of these are not relevant to conservation biology, and we are currently using OAI for access to just one provider <a href='https://bioone.org'>BioOne</a>. Journals published by BioOne that we're using as relevant to BirdLife are the following. (The count is the number of articles, from OAI access, currently in the BlitScan database.)


```{r oai-bioone-count, echo = FALSE, warning = FALSE}

df_tx_searchterm <- df_tx %>%
  select(link, search_term) %>%
  distinct() %>%
  collect()

df_tx_oai <- df_tx_searchterm %>%
  filter(str_detect(search_term, '^OAI')) %>%
  transmute(Society = str_remove(search_term, '^OAI: ')) %>%
  group_by(Society) %>%
  summarise(Count = n())

df_oai <- tbl(conn, 'oai') %>%
  collect()

inner_join(df_oai, df_tx_oai, by = 'Society') %>%
  arrange(Journal) %>%
  knitr::kable(format = "html", 
               format.args = list(big.mark = ','),
               table.attr = "style='width:100%;'")
```


#### Step 3: preprint archives

Similar to step 1, except that instead of using a commercial search engine, queries are made directly to a set of preprint archives, currently consisting of:

```{r archive-scanning, echo = FALSE, warning = FALSE}

df_archiv <- tbl(conn, 'archiv') %>%
  collect() %>%
  mutate(label = str_to_lower(str_remove(Archive, '-'))) %>%
  collect()

labels <- unique(df_archiv$label) 
simple_st <- function(st){
  labels[
    which(str_detect(st, labels))
    ]
}

df_tx_archiv <- df_tx_searchterm %>%
  filter( str_detect(search_term, paste(labels, collapse='|')) ) 
df_tx_archiv['label'] <- sapply(df_tx_archiv$search_term, simple_st) 
df_tx_archiv <- df_tx_archiv %>%
  group_by(label) %>%
  summarise(Count = n())
  
inner_join(df_archiv, df_tx_archiv, by = 'label') %>%
  select(Archive, Language, URL, Count) %>%
  knitr::kable(format = "html", 
               format.args = list(big.mark = ','),
               table.attr = "style='width:100%;'")
```

<p> In the URLs above, 'search_term' is selected from a <a href="https://github.com/billoxbury/blitscan/blob/main/data/searchterms_restricted.csv">list of scientific genus names containing vulnerable species</a> (that is, having red-list status not _LC_ or _EX_).

URLs returned from this process are stored and mined individually after all of steps 1-4 have completed. 


#### Step 4: journal contents listings

To add completeness beyond the quite general searches above, this step explicitly scans the tables of contents for recent issues of key journals. Those currently scanned are the following.

```{r scanned-journals, echo = FALSE, warning = FALSE}

normal_plos <- function(str){
  if(str_detect(str, 'PLOS')) {  'PLOS' } else { str }
}

df_tx_searchterm <- df_tx %>%
  select(search_term) %>%
  collect() %>%
  transmute(SearchPrefix = sapply(search_term, normal_plos)) 

journals <- tbl(conn, 'contentscans') %>%
  select('Journal', 'SearchPrefix', 'Language', 'URL') %>%
  collect()

inner_join(df_tx_searchterm, journals, by ='SearchPrefix') %>%
  group_by(Journal, Language, URL) %>%
  summarise(Count = n(), .groups = 'keep') %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```

#### Step 1 revisited - domains and issues arising

The initial custom search covers many more journals than those visited in steps 2-4 (see the appendix). This is possible by running searches against _domains_ rather than individual journals or even publishers:

```{r custom-search, echo = FALSE, warning = FALSE}

domain_list <- tbl(conn, 'domains') %>%
  select(domain, minable) %>%
  collect()

domain_ct <- df_tx %>%
  group_by(domain) %>%
  summarize(Count = n(), .groups = 'keep') %>%
  arrange(desc(Count)) %>%
  collect()

df_out <- full_join(domain_list, domain_ct, by='domain') %>%
  arrange(desc(Count)) %>%
  filter(!str_detect(domain, 'bioone') & !is.na(minable))
  
df_out$Count[is.na(df_out$Count)] <- 0
names(df_out) <- c('Domain', 'Minable?','Count')

df_out %>%
  knitr::kable(format = "html", 
               format.args = list(big.mark = ','),
               table.attr = "style='width:50%;'")
```

<p>The binary indicator 'minable' refers to domain policy which in some cases (minable = 0) prohibits or limits text extraction later on. Notable examples are _conbio.onlinelibrary.wiley.com_ and _academic.oup.com_. In some cases we are able to work with these domains via open archive initiatives.


### Language coverage

We aim to grow the coverage of non-English language sources by taking advantage of cloud translation services.The current distribution of languages represented in the BlitScan database is as follows:

```{r languages, echo=FALSE}

lang_all <- df %>% 
  filter(GOTTEXT == 1) %>%
  group_by(language) %>%
  summarize(OutOf = n())
lang_used <- df_tx %>% 
  group_by(language) %>%
  summarize(Count = n())

df_out <- full_join(lang_used, lang_all, by = 'language') %>%
  collect()
df_out$Count[is.na(df_out$Count)] <- 0

df_out %>%
  arrange(desc(Count)) %>%
  knitr::kable(format = "html", 
               format.args = list(big.mark = ','),
               table.attr = "style='width:50%;'")
```
<p> In this table, 'Count' refers to the current database while 'OutOf' refers to the wider data set of articles that is stored before filtering down to the database used. Filtering is based on both date and relevance. 


### Date range

The range and distribution of known dates of articles in the database:

```{r date-range, echo=FALSE}

date <- df_tx %>%
  select(date) %>%
  collect() %>%
  mutate(date = as_date(date)) 

cat(
sprintf("Date range %s to %s\n",
min(date$date, na.rm = TRUE),
max(date$date, na.rm = TRUE)
))
```

```{r date-profile-all, fig.height=3, fig.width=8, echo=FALSE, warning=FALSE}

date %>%
  filter(!is.na(date)) %>% 
  ggplot(aes(date)) +
  geom_histogram(binwidth = 7,
                 colour = "blue",
                 fill = "lightblue") +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  #scale_x_date(date_breaks = "years", date_labels = "%Y") +
  labs(x = "Publication date (weekly bins)",
       y = "Article count")
```
<p>
Note that points beyond today's date usually indicate instances where publication comes later than online announcement. 

Reading of dates can be a fallible process - how well are we able to parse publication dates for URLs?

```{r bad-dates, echo=FALSE}

baddate <- (is.na(date))
cat(sprintf("Proportion of records with date parsed: %g%%\n",
            100 * ( 1 - sum(baddate) / nr ) 
            %>% round(3)
))
```

By domain this looks like:

```{r bad-date-per-domain, echo=FALSE}
# what are the items without dates?

domain <- df_tx %>%
  select(domain) %>%
  collect() %>%
  pull(domain) %>%
  str_remove('^www\\.')

df_out <- table(domain, !baddate) %>%
  tibble()

df_out[[1]]
```



### Score distribution

Text retrieved is scored in the downstream _process_ stage. Here is the overall distribution, with the quartiles/median marked in red.

```{r score-distribution, fig.height=3, fig.width=4, echo=FALSE, warning=FALSE}

df_score <- df_tx %>%
  select(score, domain) %>%
  filter(score > -20.0) %>%
  collect()

qs <- quantile(df_score$score, na.rm=TRUE)

df_score %>% 
  ggplot(aes(score)) +
  geom_histogram(binwidth = 0.4,
                 colour = "blue",
                 fill = "lightblue") +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") +
  labs(x = "Relevance score",
       y = "URL count")
```

This score then provides a basis for comparing domains. (Note that the red quartile markers are those for the overall distribution and are the same for all subplots.) For clarity we only show domains exceeding a count threshold:

```{r plot-threshold, echo=FALSE, warning=FALSE}
# how does score distribution vary with domain?

threshold <- 30

cat("Count threshold for plot: ", threshold)
```

```{r score-per-domain, fig.height=6, fig.width=8, echo=FALSE, warning=FALSE}
# how does score distribution vary with domain?

df_score %>% 
  group_by(domain) %>%
  mutate(count = n()) %>%
  filter(count > threshold) %>%
  ungroup() %>%
  mutate(domain_updated = paste0(domain, " (", count, ")")) %>%
  ggplot(aes(score)) +
  geom_histogram(aes(score, after_stat(density)),
                 binwidth = 0.4,
                 colour = "blue",
                 fill = "lightblue") +
  #ylim(c(0,1.5)) +
  facet_wrap(~domain_updated) +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") 
```

<p> One can make some observations from this plot. For each domain, a skew to the right is good, to the left is not good.

First, the value of the OAI domain _bioone.org/action/oai_ is showing clearly. (This represents the journals listed under 'step 2' above.) (Note that the domain _bioone.org_ was mined directly for just the Wilson Journal prior to use of OAI.)

Second, the profile for _biorxiv.org_ is less strong and could suggest a review of how the site is crawled.

Third, bimodal distributions (i.e. showing outlying lumps to the left) can indicate non-English text that has not yet been translated.

Finally, a visual check on _link.springer.com_ is useful just because Springer turns out to dominate the journals visited by the custom search (step 1). (See the appendix.)

### Text retrieval

All of the above statistics refer to articles in the Blitscan database for which text has been successfully retrieved. But that represents a subset of a larger dataset of URLs (coming from steps 1-4 above) for which text retrieval was attempted. Here we summarise the success or not, by domain, of text retrieval on this fuller dataset.

We've indicated above ('Step 1 revisited') that not all domains consistently permit web crawling. Among responsive domains, the following counts show URLs from which text was retrieved (1) versus not (0):

```{r compare-responsive-domains, echo=FALSE}
# which (responsive) domains can I get text from?

problem_domains <- df %>% 
  group_by(domain) %>%
  summarise(count = sum(GOTTEXT, na.rm = TRUE)) %>%
  filter(count == 0) %>%
  pull(domain) 

domain <- df %>% 
  filter(!(domain %in% problem_domains))%>% 
  pull(domain) %>%
  str_remove('^www\\.')
  
text <- df %>% 
  filter(!(domain %in% problem_domains))%>% 
  pull(GOTTEXT)

table(domain, text)
```

### Appendix: list of all journals/proceedings

The listing below makes use of the open source project <a href="https://www.crossref.org/">CrossRef</a>. At each of the steps in the _scrape_ process we record the <a href="https://www.doi.org/">Digital Object Identifier (DOI)</a> of each article if possible. 

```{r proportion-dois, echo=FALSE}

gotdoi <- df_tx %>%
  group_by(is.na(doi)) %>%
  summarise(count = n()) %>%
  collect() %>%
  pull(count)
  
cat(sprintf("Proportion of articles with known DOI: %g%%",
            round( 100 * gotdoi[1] / nr, 3)))
```

<p>
CrossRef then allows us to query all of the known DOIs in the Blitscan database to obtain a rich set of metadata. The list below is extracted from that set:

```{r journals-via-crossref, echo=FALSE}

inner_join(df_dois, df_tx, by = 'doi') %>%
  select(Journal = container.title, Publisher = publisher) %>%
  filter(!is.na(Journal)) %>%
  group_by(Journal, Publisher) %>%
  summarise(Count = n(), .groups = 'keep') %>%
  arrange(desc(Count)) %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```

### R and package versions used

```{r sessionInfo, include=TRUE, echo=TRUE, results='markup', echo=FALSE}
sessionInfo()
```

