---
title: "Dashboard for BirdLife LitScan scraper"
author: "Bill Oxbury"
date: "`r Sys.Date()`"
output: html_document
---

_This file is updated automatically as part of the scraper processing._

```{r set-up-load-data, include=FALSE, warning=FALSE}

library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(readr)
library(lubridate)
library(ggplot2)

masterfile <- "../data/master.csv"
df_master <- read_csv(masterfile, show_col_types = FALSE)
df_master <- df_master %>% mutate(date = as_date(date),
                    query_date = as_date(query_date)) 
df_master$domain <- df_master$domain %>% str_remove('^www\\.')

txfile <- "../data/tx-master.csv"
df_tx <- read_csv(txfile, show_col_types = FALSE)
df_tx <- df_tx %>% mutate(date = as_date(date),
                    query_date = as_date(query_date)) 
df_tx$domain <- df_tx$domain %>% str_remove('^www\\.')
df_tx$doi <- df_tx$doi %>% str_remove('^doi\\:')

doifile <- "../data/doi_data_cr.csv"
df_doi <- read_csv(doifile, show_col_types = FALSE)

# SCRATCH SPACE

# after any manual adjustments:
# df %>% write_csv(masterfile)

```

### First statistics

The
<a href="http://blitscan.uksouth.azurecontainer.io:3838/">BlitScan app</a> is built over a data set of journal articles/research publications with the following basic stats:

```{r nr-records, echo=FALSE, warning=FALSE}

df <-  df_tx
cat("Number of articles:", nrow(df), '\n')
```

```{r nr-species, echo=FALSE, warning=FALSE}

sp_list <- df$species[!is.na(df$species)] %>%
  str_split('\\|') %>%
  unlist() %>%
  unique()

cat("Number of species:", length(sp_list), '\n')
```

```{r nr-journals, echo=FALSE, warning=FALSE}


# used in step 4 below
journalsfile <- "../data/scan_journal_sources.csv"
scan_journal_sources <- read_csv(journalsfile, 
                         show_col_types = FALSE)
scan_journal_sources['Count'] <- sapply(1:nrow(scan_journal_sources), function(i){
  lab <- scan_journal_sources$SearchPrefix[i] 
  # return
  sum(str_detect(df$search_term, lab))
})

# using DOIs
doi_index <- sapply(df$doi, function(d){
  which(df_doi$doi == d) 
}) %>% as.numeric()
tx_journals <- df_doi[doi_index, ] %>%
  filter(!is.na(container.title)) %>%
  group_by(container.title, publisher) %>%
  summarise(count = n(), .groups = 'keep') %>%
  ungroup()
names(tx_journals) <- c('Journal','Publisher','Count')

# combine the two
scanned <- scan_journal_sources %>%
  select(Journal, Count)
tmp <- full_join(tx_journals, scanned, by = 'Journal') 
tmp$Count.x[is.na(tmp$Count.x)] <- 0
tmp$Count.y[is.na(tmp$Count.y)] <- 0
tx_journals <- tmp %>%
  rowwise() %>%
  mutate(Count = max(Count.x, Count.y)) %>%
  select(Journal, Publisher, Count)

# a hack until correct publishers are included in '../data/scan_journal_sources.csv'
tx_journals$Publisher[is.na(tx_journals$Publisher)] <- ''

cat("Number of journals/proceedings:", nrow(tx_journals), '\n')
```

```{r nr-publishers, echo=FALSE, warning=FALSE}

tx_publisher <- df_doi$publisher[doi_index] 

np <- tx_publisher[!is.na(tx_publisher)] %>% 
  unique() %>% length()

cat("Number of publishers:", np, '\n')
```

More detail on journals and publishers is given below. A full list of journals/proceedings the database draws from is given in the appendix.


### Red-list coverage

A direct way to measure the utility of the tool is via coverage of species, classified by red-list status. The counts in the table below are the numbers of species (normalised to _SISRecID_ identifier) occuring in any title or abstract of the database, versus those occuring in the <a href="http://datazone.birdlife.org/home">BirdLife DataZone</a>.

```{r red-list, echo=FALSE, warning=FALSE}

redlistfile <- "../data/master-BLI-11107.csv"
df_redlist <- read_csv(redlistfile, 
                       col_types = cols(SISRecID = col_character())) %>%
  select(SISRecID, status)
# status counts
df_all <- df_redlist %>%
  filter(!is.na(status)) %>%
  group_by(status) %>%
  summarise(OutOf = n())

# status counts for the blitscan list
df_tmp <- tibble(SISRecID = sp_list)
df_blit <- inner_join(df_tmp, df_redlist, by = 'SISRecID') %>%
  filter(!is.na(status)) %>%
  group_by(status) %>%
  summarise(Count = n())

# join the two
df_out <- full_join(df_blit, df_all, by = 'status') %>%
  mutate(Percent = round( 100*Count/OutOf, 1)) %>%
  rename(Status = status) %>%
  arrange(desc(OutOf))

df_out %>%
  knitr::kable(format = "html", table.attr = "style='width:50%;'")
```
<p>
(Note that the numbers in the _Count_ column are underestimates and may not add up to the species count given above - this is because red-list status cannot always be ascertained. The numbers in the _OutOf_ column are not using DataZone data directly, but are estimated by a crawl of DataZone pages.)

### Sources

The search process to find web content follows four main steps to identify articles, followed by text retrieval from those articles. The four steps are:

1. Bing Custom Search against targeted domains 
2. Scanning of <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">Open Archives Initiative</a> API to get titles/abstracts from relevant contributors
3. Query of preprint archives against genus (Latin) names in vulnerable categories
4. Scanning of content lists for targeted journals.

We will summarise each of these in turn.

#### Step 1: <a href="https://docs.microsoft.com/en-us/bing/search-apis/bing-custom-search/overview">Custom Search</a>

This first step uses the full power of customised web search - previously using Google, now using Microsoft Bing. It gives us a wide reach, while the journal scans in the later steps ensure greater completeness.

The process runs around 1000 queries against a set of targeted domains (which will be listed below after describing steps 2,3,4). Each query uses a random search term taken from a <a href="https://github.com/billoxbury/blitscan/blob/main/data/searchterms_general.txt">list of generic scientific/common bird names</a> (while checking that the search hasn't been used too recently).

URLs returned from this process are stored and mined individually *after* all of steps 1-4 have completed. 

#### Step 2: <a href="http://www.openarchives.org/OAI/openarchivesprotocol.html">Open Archives Initiative (OAI)</a>

OAI is an invaluable centralised service that collects research metadata, including titles/abstracts, from institutions and publishers around the world. On the one hand, it protects these providers from cyber threats if they permit web crawling directly to their servers; on the other hand, it greatly simplifies the process for us, as data consumers, in providing a single format across a very wide community of providers.

As of June 2022 there are 5,616 listed providers to OAI. Most of these are not relevant to conservation biology, and we are current using OAI for access to just one provider <a href='https://bioone.org'>BioOne</a>. Journals published by BioOne that we're using as relevant to BirdLife are the following. (The count is the number of articles, from OAI access, currently in the BlitScan database.)


```{r oai-bioone-count, echo = FALSE, warning = FALSE}

oaifile <- '../data/oai_bioone_sources.csv'
df_oai <- read_csv(oaifile, show_col_types = FALSE)

idx <- doi_index[str_detect(df$search_term, 'OAI')]

df_bioone <- df_doi[idx,] %>%
  filter(!is.na(container.title), preserve = TRUE) %>%
  group_by(container.title) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) 
names(df_bioone) <- c('Journal','Count')

df_bioone <- full_join(df_oai, df_bioone, by='Journal') %>%
  arrange(Journal)

df_bioone %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```

#### Step 3: preprint archives

This is similar to step 1, except that instead of using a commercial search engine, queries are made directly to a set of preprint archives, currently consisiting of:

```{r archive-scanning, echo = FALSE, warning = FALSE}

df <- df_tx

archivfile <- '../data/scan_archive_sources.csv'
df_archiv <- read_csv(archivfile, show_col_types = FALSE)

labs <- df_archiv$Archive %>% 
  str_remove('-') %>% 
  str_to_lower()

df_archiv['Count'] <- sapply(1:nrow(df_archiv), function(i){
  lab <- df_archiv$Archive[i] %>% 
    str_remove('-') %>% 
    str_to_lower()
  # return
  sum(str_detect(df$search_term, lab))
})

df_archiv %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```
<p> In the URLs above, 'search_term' is selected from a <a href="https://github.com/billoxbury/blitscan/blob/main/data/searchterms_restricted.csv">list of scientific genus names containing vulnerable species</a> (that is, having red-list status not _LC_ or _EX_).

URLs returned from this process are stored and mined individually after all of steps 1-4 have completed. 


#### Step 4: journal contents listings

To add completeness beyond the quite general searches above, this step explicitly scans the tables of contents for recent issues of relevant journals. Journals currently scanned are the following.

```{r scanned-journals, echo = FALSE, warning = FALSE}


scan_journal_sources[,c(1,2,3,5)] %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```
#### Step 1 revisited - domains and issues arising

The initial custom search covers many more journals than those visited in steps 2-4 (see the appendix). This is possible by running searches against _domains_ rather than individual journals or even publishers. Here is the current list of domains:

```{r custom-search, echo = FALSE, warning = FALSE}

domainfile <- "../data/xpath_rules.csv"
df_domain <- read_csv(domainfile, show_col_types = FALSE)
domain_list <- df_domain[,1:2] 

domain_ct <- df %>%
  group_by(domain) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count))

df_out <- full_join(domain_list, domain_ct, by='domain') %>%
  arrange(desc(Count)) %>%
  filter(!str_detect(domain, 'bioone'))
df_out$Count[is.na(df_out$Count)] <- 0

names(df_out) <- c('Domain','Minable?','Count')
df_out %>%
  knitr::kable(format = "html", table.attr = "style='width:50%;'")
```

<p>The binary indicator 'minable' refers to domain policy which in some cases (minable = 0) prohibits or limits text extraction later on. Notable examples are _conbio.onlinelibrary.wiley.com_ and _academic.oup.com_. In some cases we are able to work with these domains via open archive initiatives.


### Language coverage

We aim to grow the coverage of non-English language sources by taking advantage of cloud translation services.The current distribution of languages represented in the BlitScan database is as follows:

```{r languages, echo=FALSE}


lang_all <- df_master %>% 
  filter(GOTTEXT == 1) %>%
  group_by(language) %>%
  summarize(OutOf = n())
lang_used <- df_tx %>% 
  group_by(language) %>%
  summarize(Count = n())

df_out <- full_join(lang_used, lang_all, by = 'language')
df_out$Count[is.na(df_out$Count)] <- 0

df_out %>%
  knitr::kable(format = "html", table.attr = "style='width:50%;'")
```
<p> In this table, 'Count' refers to the current database while 'OutOf' refers to the wider data set of articles that is stored before filtering down to the database used. Filtering is based on both date and relevance. 


### Date range

The range and distribution of known dates of articles in the database:

```{r date-range, echo=FALSE}

df <- df_tx

cat(
sprintf("Date range %s to %s\n",
min(df$date, na.rm = TRUE),
max(df$date, na.rm = TRUE)
))
```

```{r date-profile-all, fig.height=3, fig.width=8, echo=FALSE, warning=FALSE}

baddate <- (is.na(df$date))
df[!baddate,] %>% 
  ggplot(aes(date)) +
  geom_histogram(binwidth = 7,
                 colour = "blue",
                 fill = "lightblue") +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  #scale_x_date(date_breaks = "years", date_labels = "%Y") +
  labs(x = "Publication date (weekly bins)",
       y = "Article count")
```
<p>
Note that points beyond today's date may occasionally indicate date errors but in most cases arise where publication comes later than online announcement. 

Reading of dates can be a fallible process - how well are we able to parse publication dates for URLs?

```{r bad-dates, echo=FALSE}

df <- df_tx

baddate <- (is.na(df$date))
cat(sprintf("Proportion of records with date parsed: %g%%\n",
            100 * ( 1 - sum(baddate) / nrow(df)) 
            %>% round(1)
))
```

By domain this looks like:

```{r bad-date-per-domain, echo=FALSE}
# what are the items without dates?

df <- df_tx

good_date <- !baddate
df_out <- table(df$domain, good_date) %>%
  tibble()
df_out[[1]]
```



### Score distribution

Text retrieved is scored in the downstream _process_ stage. From the point of view of the _scrape_ service, it doesn't matter how it is calculated but is a black-box feedback signal representing 'customer satisfaction' with the text output by _scrape_.

Here is the overall distribution, with the quartiles/median marked in red.

```{r score-distribution, fig.height=3, fig.width=4, echo=FALSE, warning=FALSE}

df <- df_tx

qs <- quantile(df$score[df$score > -20.0], na.rm=TRUE)

df[df$score > -20,] %>% 
  ggplot(aes(score)) +
  geom_histogram(binwidth = 0.4,
                 colour = "blue",
                 fill = "lightblue") +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") +
  labs(x = "Relevance score",
       y = "URL count")
```

This score then provides a basis for comparing domains. (Note that the red quartile markers are those for the overall distribution and are the same for all subplots.) For clarity we only show domains exceeding a count threshold:

```{r plot-threshold, echo=FALSE, warning=FALSE}
# how does score distribution vary with domain?

threshold <- 30

cat("Count threshold for plot: ", threshold)
```

```{r score-per-domain, fig.height=6, fig.width=8, echo=FALSE, warning=FALSE}
# how does score distribution vary with domain?

df[df$score > -20 & !is.na(df$score),] %>% 
  group_by(domain) %>%
  mutate(count = n()) %>%
  filter(count > threshold) %>%
  ungroup() %>%
  mutate(domain_updated = paste0(domain, " (", count, ")")) %>%
  ggplot(aes(score)) +
  geom_histogram(aes(score, after_stat(density)),
                 binwidth = 0.4,
                 colour = "blue",
                 fill = "lightblue") +
  #ylim(c(0,1.5)) +
  facet_wrap(~domain_updated) +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") 
```

<p> One can make some observations from this plot. For each domain, a skew to the right is good, to the left is not good.

First, the value of the OAI domain _bioone.org/action/oai_ is showing clearly. (This represents the journals listed under 'step 2' above.) (Note that the domain _bioone.org_ was mined directly for just the Wilson Journal prior to use of OAI.)

Second, the profile for _biorxiv.org_ is less strong and could suggest a review of how the site is crawled.

Third, bimodal distributions (i.e. showing outlying lumps to the left) can indicate non-English text that has not yet been translated.

Finally, a visual check on _link.springer.com_ is useful just because Springer turns out to dominate the journals visited by the custom search (step 1). (See the appendix.)

### Text retrieval

All of the above statistics refer to articles in the Blitscan database for which text has been successfully retrieved. But that represents a subset of a larger dataset of URLs (coming from steps 1-4 above) for which text retrieval was attempted. Here we summarise the success or not, by domain, of text retrieval on this fuller dataset.

We've indicated above ('Step 1 revisited') that not all domains consistently permit web crawling. Among responsive domains, the following counts show URLs from which text was retrieved (1) versus not (0):

```{r compare-responsive-domains, echo=FALSE}
# which (responsive) domains can I get text from?

df <- df_master

problem_domains <- df %>% 
  group_by(domain) %>%
  summarise(count = sum(GOTTEXT)) %>%
  filter(count == 0) %>%
  select(domain) 

df <- df[!(df$domain %in% problem_domains$domain),]
table(df$domain, df$GOTTEXT)
```

### Appendix: list of all journals/proceedings

The listing below makes use of the open source project <a href="https://www.crossref.org/">CrossRef</a>. At each of the steps in the _scrape_ process we record the <a href="https://www.doi.org/">Digital Object Identifier (DOI)</a> of each article if possible. 

```{r proportion-dois, echo=FALSE}

df <- df_tx

cat(sprintf("Proportion of articles with known DOI: %g%%",
            round( 100 * sum(!is.na(df$doi)) / nrow(df))))
```

<p>
CrossRef then allows us to query all of the known DOIs in the Blitscan database to obtain a rich set of metadata. The list below is extracted from that set:

```{r journals-via-crossref, echo=FALSE}

tx_journals %>%
  arrange(desc(Count)) %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```

### R and package versions used

```{r sessionInfo, include=TRUE, echo=TRUE, results='markup', echo=FALSE}
sessionInfo()
```

