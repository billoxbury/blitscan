---
title: "Dashboard for BirdLife LitScan scraper"
author: "Bill Oxbury"
date: "`r Sys.Date()`"
output: html_document
---

_This file is generated automatically as part of the scraper processing._

```{r set-up-load-data, include=FALSE, warning=FALSE}

library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(readr)
library(lubridate)
library(ggplot2)

datafile <- sprintf("../data/bing-master.csv") 
df <- read_csv(datafile, show_col_types = FALSE)
df <- df %>% mutate(date = as_date(date),
                    query_date = as_date(query_date)) 
df$domain <- df$domain %>% str_remove('^www\\.')
```

### Journals and domains

As well as using Bing Custom Search to target domains (such as _link.springer.com_) we also directly scan recent 
issues of specific journals (such as _www.springer.com/journal/10336_, Journal of Ornithology). The custom search 
gives us wider reach, but the journal scans ensure greater completeness.

Journals currently scanned are the following.

```{r scanned-journals, echo = FALSE, warning = FALSE}
scan_sources <- read_csv("../data/scan_sources.csv", 
                         show_col_types = FALSE)
scan_sources %>%
  knitr::kable(format = "html", table.attr = "style='width:100%;'")
```

### Text retrieval

The following domains _either_ did not permit any text or pdf scraping _or_ do not yet have xpath rules encoded and will be excluded from the analysis below:

```{r problem-domains, echo=FALSE}
# which domains don't respond?

problem_domains <- df %>% 
  group_by(domain) %>%
  summarise(count = sum(GOTTEXT)) %>%
  filter(count == 0) %>%
  select(domain) 
for(d in problem_domains$domain){
  cat(d, '\n')
}
```

Among responsive domains, the following counts show URLs from which text was retrieved (1) versus not (0):

```{r compare-responsive-domains, echo=FALSE}
# which (responsive) domains can I get text from?

# FOR NOW:
df <- df[!(df$domain %in% problem_domains$domain),]
table(df$domain, df$GOTTEXT)
```

Current distribution of texts by original language:

```{r languages, echo=FALSE}

cat('Total:\n')
df$language[df$GOTTEXT == 1] %>% 
  table()

cat('By domain:\n')
table(df$domain[df$GOTTEXT == 1], df$language[df$GOTTEXT == 1])
```


Text retrieval success (1 versus 0) by query date:

```{r compare-with-query-date, echo=FALSE}
# correlation with query date
#table(df$query_date, df$GOTTEXT)

cat('Removed for graphic - in progress\n')
```

How successful is text retrieval from PDF versus from HTML? (Noting that HTML gets the first shot and if successful PDF is not attempted.)

In the following 2 x 2 table, the columns are text retieval success as above. The rows indicate whether text was retrieved at the PDF stage (1) versus not (0):

```{r how-much-pdf, echo=FALSE}
# how much or retrieved text came from PDF?
table(df$DONEPDF, df$GOTTEXT)
```

### Score distribution

Text retrieved is scored in the downstream _process_ stage. From the point of view of the _scrape_ service, it doesn't matter how it is calculated but is a black-box feedback signal representing 'customer satisfaction' with the text output by _scrape_.

Here is the overall distribution, with the quartiles/median marked in red:

```{r score-distribution, fig.height=3, fig.width=4, echo=FALSE, warning=FALSE}

qs <- quantile(df$score[df$score > -20.0], na.rm=TRUE)

df[df$score > -20,] %>% 
  ggplot(aes(score)) +
  geom_histogram(binwidth = 0.5,
                 colour = "blue",
                 fill = "lightblue") +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") +
  labs(x = "Relevance score",
       y = "URL count")
```

This score then provides a basis for comparing (responsive) domains:

```{r score-per-domain, fig.height=6, fig.width=8, echo=FALSE, warning=FALSE}
# how does score distribution vary with domain?

df[df$score > -20 & !is.na(df$score),] %>% 
  group_by(domain) %>%
  mutate(count = n()) %>%
  ungroup() %>%
  mutate(domain_updated = paste0(domain, " (", count, ")")) %>%
  ggplot(aes(score)) +
  geom_histogram(aes(score, after_stat(density)),
                 binwidth = 0.1,
                 colour = "blue",
                 fill = "lightblue") +
  ylim(c(0,1.5)) +
  facet_wrap(~domain_updated) +
  geom_rug(data = tibble(qs)[2:4,],
           aes(x = qs), 
           col = "red") 
```

Note that a bi/multimodal distribution can arise because of the presence of foreign-language text. (This has now been corrected for *ace-eco.org* by removing French text at abstract retrieval.)


### Date range

How well are we able to parse publication dates for URLs?

```{r bad-dates, echo=FALSE}

baddate <- (is.na(df$date))
cat(sprintf("Proportion of records with date parsed: %g%%\n",
            100 * ( 1 - sum(baddate) / nrow(df)) 
            %>% round(1)
))
```

By domain this looks like:

```{r bad-date-per-domain, echo=FALSE}
# what are the items without dates?

table(df$domain, baddate)
```

Where dates are readable, the full date range of articles in the database:

```{r date-range, echo=FALSE}

cat(
sprintf("Date range %s to %s\n",
min(df$date, na.rm = TRUE),
max(df$date, na.rm = TRUE)
))
```


```{r date-profile-all, fig.height=3, fig.width=8, echo=FALSE, warning=FALSE}

df[!baddate,] %>% 
  ggplot(aes(date)) +
  geom_histogram(binwidth = 365,
                 colour = "blue",
                 fill = "lightblue") +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  #scale_x_date(date_breaks = "years", date_labels = "%Y") +
  labs(x = "Publication date",
       y = "Article count")
```

Date profile filtered to dates within the past 3 years (for Bing Custom Search we see this is roughly uniform, and we're not seeing the weighting to more recent results that we got with Google Custom Search):

```{r date-profile-used, fig.height=3, fig.width=8, echo=FALSE, warning=FALSE}

df[df$BADLINK==0,] %>% ggplot(aes(date)) +
  geom_histogram(binwidth = 7,
                 colour = "blue",
                 fill = "lightblue") +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  scale_x_date(date_breaks = "months", date_labels = "%b-%Y") +
  labs(x = "Publication date (weekly bins)",
       y = "Article count")
```

Note any points beyond today's date, which may indicate date errors, or in some cases arise where publication comes later than online announcement. 

### R and package versions used

```{r sessionInfo, include=TRUE, echo=TRUE, results='markup', echo=FALSE}
sessionInfo()
```

