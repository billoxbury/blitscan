{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31826e27",
   "metadata": {},
   "source": [
    "# A bag-of-words topic model for BirdLife"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6f3ba",
   "metadata": {},
   "source": [
    "## Extracting sentences from the BirdLife text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85142aac",
   "metadata": {},
   "source": [
    "Load the text data scraped from BirdLife assessments. The cleaned text that we will use is in the field 'text_short'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5d4b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name_com</th>\n",
       "      <th>name_sci</th>\n",
       "      <th>SISRecID</th>\n",
       "      <th>date</th>\n",
       "      <th>text_main</th>\n",
       "      <th>text_short</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Cream-browed White-eye</td>\n",
       "      <td>Heleia superciliaris</td>\n",
       "      <td>22714307</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nAlthoug...</td>\n",
       "      <td>Although this species may have a restricted ra...</td>\n",
       "      <td>10.250856</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Striped Sparrow</td>\n",
       "      <td>Oriturus superciliosus</td>\n",
       "      <td>22721301</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has a very large range, and hence...</td>\n",
       "      <td>14.209733</td>\n",
       "      <td>-2.746421</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>White-chinned Prinia</td>\n",
       "      <td>Schistolais leucopogon</td>\n",
       "      <td>22713643</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>4.975323</td>\n",
       "      <td>4.849581</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Masked Water-tyrant</td>\n",
       "      <td>Fluvicola nengeta</td>\n",
       "      <td>22700284</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>6.9901</td>\n",
       "      <td>-0.280215</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Lendu Crombec</td>\n",
       "      <td>Sylvietta chapini</td>\n",
       "      <td>22715107</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species is listed as Critically Endangere...</td>\n",
       "      <td>5.344564</td>\n",
       "      <td>4.205028</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link                name_com  \\\n",
       "0  http://datazone.birdlife.org/species/factsheet...  Cream-browed White-eye   \n",
       "1  http://datazone.birdlife.org/species/factsheet...         Striped Sparrow   \n",
       "2  http://datazone.birdlife.org/species/factsheet...    White-chinned Prinia   \n",
       "3  http://datazone.birdlife.org/species/factsheet...     Masked Water-tyrant   \n",
       "4  http://datazone.birdlife.org/species/factsheet...           Lendu Crombec   \n",
       "\n",
       "                 name_sci  SISRecID        date  \\\n",
       "0    Heleia superciliaris  22714307  2022-01-31   \n",
       "1  Oriturus superciliosus  22721301  2022-01-31   \n",
       "2  Schistolais leucopogon  22713643  2022-01-31   \n",
       "3       Fluvicola nengeta  22700284  2022-01-31   \n",
       "4       Sylvietta chapini  22715107  2022-01-31   \n",
       "\n",
       "                                           text_main  \\\n",
       "0  \\n Justification of Red List Category\\nAlthoug...   \n",
       "1  \\n Justification of Red List Category\\nThis sp...   \n",
       "2  \\n Justification of Red List Category\\nThis sp...   \n",
       "3  \\n Justification of Red List Category\\nThis sp...   \n",
       "4  \\n Justification of Red List Category\\nThis sp...   \n",
       "\n",
       "                                          text_short          x         y  \\\n",
       "0  Although this species may have a restricted ra...  10.250856   -0.3714   \n",
       "1  This species has a very large range, and hence...  14.209733 -2.746421   \n",
       "2  This species has an extremely large range, and...   4.975323  4.849581   \n",
       "3  This species has an extremely large range, and...     6.9901 -0.280215   \n",
       "4  This species is listed as Critically Endangere...   5.344564  4.205028   \n",
       "\n",
       "  status  \n",
       "0     LC  \n",
       "1     LC  \n",
       "2     LC  \n",
       "3     LC  \n",
       "4     CR  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datapath = \"../../data/\"\n",
    "bli_master = datapath + \"master-BLI.csv\"\n",
    "\n",
    "df = pd.read_csv(bli_master, index_col = None).fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f286a36",
   "metadata": {},
   "source": [
    "Using <a href=\"https://spacy.io/\">spaCy</a>, we'll extract a list of distinct sentence from across all the 'text_short' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81689ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 6 --> 6\n",
      "100: 1259 --> 871\n",
      "200: 2418 --> 1631\n",
      "300: 3520 --> 2292\n",
      "400: 4803 --> 3172\n",
      "500: 5876 --> 3772\n",
      "600: 6984 --> 4453\n",
      "700: 8402 --> 5456\n",
      "800: 9736 --> 6371\n",
      "900: 11119 --> 7327\n",
      "1000: 12448 --> 8263\n",
      "1100: 13621 --> 8994\n",
      "1200: 14823 --> 9783\n",
      "1300: 16202 --> 10738\n",
      "1400: 17442 --> 11552\n",
      "1500: 19036 --> 12734\n",
      "1600: 20410 --> 13666\n",
      "1700: 21823 --> 14663\n",
      "1800: 23168 --> 15572\n",
      "1900: 24201 --> 16132\n",
      "2000: 25598 --> 17116\n",
      "2100: 26800 --> 17864\n",
      "2200: 27987 --> 18591\n",
      "2300: 29271 --> 19403\n",
      "2400: 30498 --> 20170\n",
      "2500: 31899 --> 21098\n",
      "2600: 33210 --> 21970\n",
      "2700: 34617 --> 22919\n",
      "2800: 35977 --> 23840\n",
      "2900: 37197 --> 24615\n",
      "3000: 38509 --> 25490\n",
      "3100: 39791 --> 26322\n",
      "3200: 40973 --> 27058\n",
      "3300: 42174 --> 27791\n",
      "3400: 43253 --> 28379\n",
      "3500: 44571 --> 29263\n",
      "3600: 45686 --> 29905\n",
      "3700: 46790 --> 30547\n",
      "3800: 47937 --> 31231\n",
      "3900: 49179 --> 31986\n",
      "4000: 50407 --> 32734\n",
      "4100: 51697 --> 33573\n",
      "4200: 52855 --> 34297\n",
      "4300: 54341 --> 35328\n",
      "4400: 55544 --> 36060\n",
      "4500: 56887 --> 36945\n",
      "4600: 58181 --> 37788\n",
      "4700: 59698 --> 38837\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher                                                                                                                                                                                         \n",
    "\n",
    "# load a language model\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "#Â recognise verbs\n",
    "def verbs(sent):\n",
    "    pattern=[\n",
    "        {'POS': 'VERB', 'OP': '?'},\n",
    "        {'POS': 'ADV', 'OP': '*'},\n",
    "        {'POS': 'VERB', 'OP': '+'}\n",
    "    ]\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    # add pattern to matcher\n",
    "    matcher.add('verb-phrases', [pattern])\n",
    "    d = nlp(sent.text)\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(d)\n",
    "    spans = [d[start:end] for _, start, end in matches] \n",
    "    return spans\n",
    "\n",
    "# recognise clean sentences\n",
    "def clean_sentences(sents):\n",
    "    sentences = [s for s in sents if len(verbs(s)) > 0 and\n",
    "                    len(s) > 3]\n",
    "    return sentences\n",
    "\n",
    "# build our list, called 'sentences'\n",
    "texts = list(df['text_short'])\n",
    "sentences = []\n",
    "count = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    txt = df.at[i, 'text_short']\n",
    "    doc = nlp(txt)\n",
    "    new_sents = clean_sentences(list(doc.sents))\n",
    "    count += len(new_sents)\n",
    "    sentences += [str(x) for x in new_sents]\n",
    "    if i % 100 == 0: \n",
    "        # dedupe and show progress\n",
    "        sentences = list(set(sentences))\n",
    "        print(f'{i}: {count} --> {len(sentences)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fb3b9",
   "metadata": {},
   "source": [
    "That took a while, so let's write the sentences to disk for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c7d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = datapath + \"bli_sentences.txt\"\n",
    "with open(outfile, 'w') as fp:\n",
    "    for s in sentences:\n",
    "        fp.write(s + '\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb40c9",
   "metadata": {},
   "source": [
    "## Building a topic from the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2634a0",
   "metadata": {},
   "source": [
    "Load the sentences. (If cloning from the GitHub repo, you can start here as the sentece file is included in the <i>data</i> directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fd93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/\"\n",
    "\n",
    "sentfile = datapath + \"bli_sentences.txt\"\n",
    "with open(sentfile, 'r') as fp:\n",
    "    sentences = [s.strip() for s in fp.readlines()]\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0910487",
   "metadata": {},
   "source": [
    "Load spaCy and build a list 'words'. Each entry is a list of normalised words (tokens) from a sentence in our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fd3fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20162 words in 39800 distinct sentences.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "# tags we want to remove from the text\n",
    "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "# build token list\n",
    "words = []\n",
    "for s in nlp.pipe(sentences):\n",
    "    toks = [token.lemma_.lower() for token in s\n",
    "               if token.pos_ not in removal \n",
    "               and not token.is_stop \n",
    "               and token.is_alpha]\n",
    "    words.append(toks) \n",
    "\n",
    "# check number of distinct words:\n",
    "word_set = list(set(sum(words, [])))\n",
    "print(f'Found {len(word_set)} words in {len(sentences)} distinct sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce87b08",
   "metadata": {},
   "source": [
    "We'll use the 'gensim' package to build a dictionary and track frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25718c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10121 population\n",
      " 9610 specie\n",
      " 5520 habitat\n",
      " 4892 decline\n",
      " 4794 forest\n",
      " 4348 range\n",
      " 3890 area\n",
      " 3477 individual\n",
      " 3267 estimate\n",
      " 3155 species\n",
      " 2992 breeding\n",
      " 2668 occur\n",
      " 2616 size\n",
      " 2606 bird\n",
      " 2552 small\n",
      " 2124 year\n",
      " 2087 island\n",
      " 2036 record\n",
      " 2000 find\n",
      " 1972 nest\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "bli_dictionary = Dictionary(words)\n",
    "#bli_dictionary.filter_extremes(no_below = 10, \n",
    "#                           no_above = 0.5, \n",
    "#                           keep_n = 5000)\n",
    "bli_vocab = bli_dictionary.token2id.keys()\n",
    "\n",
    "# show top 20 most frequent\n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count <= 0: break\n",
    "    print(f'{x[1]:5} {bli_dictionary[x[0]]}')\n",
    "    count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb1b8",
   "metadata": {},
   "source": [
    "We want to assign importance weightings to these words in a natural way.\n",
    "The attribute 'dfs' is the number of documents (i.e. BLI sentences) containing a given token. We will convert this into a log-likelihood of seeing the token in a sentence drawn from the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9f7baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3692544390836279 population\n",
      "-1.421062689308192 specie\n",
      "-1.9754890520013877 habitat\n",
      "-2.096265694465467 decline\n",
      "-2.1165017762782004 forest\n",
      "-2.214150943031429 range\n",
      "-2.3254577546600377 area\n",
      "-2.437697059264668 individual\n",
      "-2.499994779671459 estimate\n",
      "-2.534878416297216 species\n",
      "-2.587924852178162 breeding\n",
      "-2.7025377842370153 occur\n",
      "-2.72222047869544 size\n",
      "-2.726050433587739 bird\n",
      "-2.746989546807849 small\n",
      "-2.9305658089107 year\n",
      "-2.948139284195329 island\n",
      "-2.972879813602116 record\n",
      "-2.9907197317304473 find\n",
      "-3.004818656109949 nest\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "log_nsents = log(len(words))\n",
    "bli_loglik = dict()\n",
    "\n",
    "# show top 20 \n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    tok = bli_dictionary[x[0]]\n",
    "    bli_loglik[tok] = log(x[1]) - log_nsents\n",
    "    if count > 0:\n",
    "        print(f'{bli_loglik[tok]:5} {bli_dictionary[x[0]]}')\n",
    "        count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e28680",
   "metadata": {},
   "source": [
    "This dict is now the model which we'll use for LitScan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d589689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk \n",
    "\n",
    "import json\n",
    "\n",
    "outfile = datapath + \"bli_model.json\"\n",
    "with open(outfile, 'w') as jf:\n",
    "    json.dump(bli_loglik, jf)\n",
    "jf.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
