{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31826e27",
   "metadata": {},
   "source": [
    "# A bag-of-words topic model for BirdLife"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6f3ba",
   "metadata": {},
   "source": [
    "## Extracting sentences from the BirdLife text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85142aac",
   "metadata": {},
   "source": [
    "Load the text data scraped from BirdLife assessments. The cleaned text that we will use is in the field 'text_short'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5d4b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name_com</th>\n",
       "      <th>name_sci</th>\n",
       "      <th>SISRecID</th>\n",
       "      <th>date</th>\n",
       "      <th>text_main</th>\n",
       "      <th>text_short</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Cream-browed White-eye</td>\n",
       "      <td>Heleia superciliaris</td>\n",
       "      <td>22714307</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nAlthoug...</td>\n",
       "      <td>Although this species may have a restricted ra...</td>\n",
       "      <td>10.250856</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Striped Sparrow</td>\n",
       "      <td>Oriturus superciliosus</td>\n",
       "      <td>22721301</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has a very large range, and hence...</td>\n",
       "      <td>14.209733</td>\n",
       "      <td>-2.746421</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>White-chinned Prinia</td>\n",
       "      <td>Schistolais leucopogon</td>\n",
       "      <td>22713643</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>4.975323</td>\n",
       "      <td>4.849581</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Masked Water-tyrant</td>\n",
       "      <td>Fluvicola nengeta</td>\n",
       "      <td>22700284</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>6.9901</td>\n",
       "      <td>-0.280215</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Lendu Crombec</td>\n",
       "      <td>Sylvietta chapini</td>\n",
       "      <td>22715107</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species is listed as Critically Endangere...</td>\n",
       "      <td>5.344564</td>\n",
       "      <td>4.205028</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link                name_com  \\\n",
       "0  http://datazone.birdlife.org/species/factsheet...  Cream-browed White-eye   \n",
       "1  http://datazone.birdlife.org/species/factsheet...         Striped Sparrow   \n",
       "2  http://datazone.birdlife.org/species/factsheet...    White-chinned Prinia   \n",
       "3  http://datazone.birdlife.org/species/factsheet...     Masked Water-tyrant   \n",
       "4  http://datazone.birdlife.org/species/factsheet...           Lendu Crombec   \n",
       "\n",
       "                 name_sci  SISRecID        date  \\\n",
       "0    Heleia superciliaris  22714307  2022-01-31   \n",
       "1  Oriturus superciliosus  22721301  2022-01-31   \n",
       "2  Schistolais leucopogon  22713643  2022-01-31   \n",
       "3       Fluvicola nengeta  22700284  2022-01-31   \n",
       "4       Sylvietta chapini  22715107  2022-01-31   \n",
       "\n",
       "                                           text_main  \\\n",
       "0  \\n Justification of Red List Category\\nAlthoug...   \n",
       "1  \\n Justification of Red List Category\\nThis sp...   \n",
       "2  \\n Justification of Red List Category\\nThis sp...   \n",
       "3  \\n Justification of Red List Category\\nThis sp...   \n",
       "4  \\n Justification of Red List Category\\nThis sp...   \n",
       "\n",
       "                                          text_short          x         y  \\\n",
       "0  Although this species may have a restricted ra...  10.250856   -0.3714   \n",
       "1  This species has a very large range, and hence...  14.209733 -2.746421   \n",
       "2  This species has an extremely large range, and...   4.975323  4.849581   \n",
       "3  This species has an extremely large range, and...     6.9901 -0.280215   \n",
       "4  This species is listed as Critically Endangere...   5.344564  4.205028   \n",
       "\n",
       "  status  \n",
       "0     LC  \n",
       "1     LC  \n",
       "2     LC  \n",
       "3     LC  \n",
       "4     CR  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datapath = \"../../data/\"\n",
    "bli_master = datapath + \"master-BLI-11107.csv\"\n",
    "\n",
    "# the suffix 11107 is the number of species in the data frame\n",
    "\n",
    "df = pd.read_csv(bli_master, index_col = None).fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f286a36",
   "metadata": {},
   "source": [
    "Using <a href=\"https://spacy.io/\">spaCy</a>, we'll extract a list of distinct sentence from across all the 'text_short' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81689ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 6 --> 6\n",
      "500: 5876 --> 3772\n",
      "1000: 12448 --> 8263\n",
      "1500: 19036 --> 12734\n",
      "2000: 25598 --> 17116\n",
      "2500: 31899 --> 21098\n",
      "3000: 38509 --> 25490\n",
      "3500: 44571 --> 29263\n",
      "4000: 50407 --> 32734\n",
      "4500: 56887 --> 36945\n",
      "5000: 63580 --> 41319\n",
      "5500: 69746 --> 45191\n",
      "6000: 76384 --> 49520\n",
      "6500: 84476 --> 55540\n",
      "7000: 93551 --> 62462\n",
      "7500: 103195 --> 69869\n",
      "8000: 109954 --> 74205\n",
      "8500: 115461 --> 77119\n",
      "9000: 119839 --> 78991\n",
      "9500: 125234 --> 82063\n",
      "10000: 130274 --> 84643\n",
      "10500: 135180 --> 87115\n",
      "11000: 140673 --> 90070\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher                                                                                                                                                                                         \n",
    "\n",
    "# load a language model\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "#Â recognise verbs\n",
    "def verbs(sent):\n",
    "    pattern=[\n",
    "        {'POS': 'VERB', 'OP': '?'},\n",
    "        {'POS': 'ADV', 'OP': '*'},\n",
    "        {'POS': 'VERB', 'OP': '+'}\n",
    "    ]\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    # add pattern to matcher\n",
    "    matcher.add('verb-phrases', [pattern])\n",
    "    d = nlp(sent.text)\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(d)\n",
    "    spans = [d[start:end] for _, start, end in matches] \n",
    "    return spans\n",
    "\n",
    "# recognise clean sentences\n",
    "def clean_sentences(sents):\n",
    "    sentences = [s for s in sents if len(verbs(s)) > 0 and\n",
    "                    len(s) > 3]\n",
    "    return sentences\n",
    "\n",
    "# build our list, called 'sentences'\n",
    "texts = list(df['text_short'])\n",
    "sentences = []\n",
    "count = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    txt = df.at[i, 'text_short']\n",
    "    doc = nlp(txt)\n",
    "    new_sents = clean_sentences(list(doc.sents))\n",
    "    count += len(new_sents)\n",
    "    sentences += [str(x) for x in new_sents]\n",
    "    if i % 500 == 0: \n",
    "        # dedupe and show progress\n",
    "        sentences = list(set(sentences))\n",
    "        print(f'{i}: {count} --> {len(sentences)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fb3b9",
   "metadata": {},
   "source": [
    "That took a while, so let's write the sentences to disk for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c7d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = datapath + \"bli_sentences_11107.txt\"\n",
    "with open(outfile, 'w') as fp:\n",
    "    for s in sentences:\n",
    "        fp.write(s + '\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb40c9",
   "metadata": {},
   "source": [
    "## Building a topic from the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2634a0",
   "metadata": {},
   "source": [
    "Load the sentences. (If cloning from the GitHub repo, you can start here as the sentece file is included in the <i>data</i> directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fd93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/\"\n",
    "\n",
    "sentfile = datapath + \"bli_sentences_11107.txt\"\n",
    "with open(sentfile, 'r') as fp:\n",
    "    sentences = [s.strip() for s in fp.readlines()]\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0910487",
   "metadata": {},
   "source": [
    "Load spaCy and build a list 'words'. Each entry is a list of normalised words (tokens) from a sentence in our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fd3fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31489 words in 91070 distinct sentences.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "# tags we want to remove from the text\n",
    "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "# build token list\n",
    "words = []\n",
    "for s in nlp.pipe(sentences):\n",
    "    toks = [token.lemma_.lower() for token in s\n",
    "               if token.pos_ not in removal \n",
    "               and not token.is_stop \n",
    "               and token.is_alpha]\n",
    "    words.append(toks) \n",
    "\n",
    "# check number of distinct words:\n",
    "word_set = list(set(sum(words, [])))\n",
    "print(f'Found {len(word_set)} words in {len(sentences)} distinct sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce87b08",
   "metadata": {},
   "source": [
    "We'll use the 'gensim' package to build a dictionary and track frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25718c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22913 population\n",
      "22042 specie\n",
      "12508 habitat\n",
      "11148 forest\n",
      "11101 decline\n",
      "10133 range\n",
      " 8882 area\n",
      " 8048 individual\n",
      " 7335 estimate\n",
      " 7135 species\n",
      " 6689 breeding\n",
      " 6256 occur\n",
      " 5861 small\n",
      " 5707 size\n",
      " 5693 bird\n",
      " 4984 record\n",
      " 4910 year\n",
      " 4734 island\n",
      " 4647 find\n",
      " 4584 nest\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "bli_dictionary = Dictionary(words)\n",
    "#bli_dictionary.filter_extremes(no_below = 10, \n",
    "#                           no_above = 0.5, \n",
    "#                           keep_n = 5000)\n",
    "bli_vocab = bli_dictionary.token2id.keys()\n",
    "\n",
    "# show top 20 most frequent\n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count <= 0: break\n",
    "    print(f'{x[1]:5} {bli_dictionary[x[0]]}')\n",
    "    count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb1b8",
   "metadata": {},
   "source": [
    "We want to assign importance weightings to these words in a natural way.\n",
    "The attribute 'dfs' is the number of documents (i.e. BLI sentences) containing a given token. We will convert this into a log-likelihood of seeing the token in a sentence drawn from the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f7baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.379924006502522 population\n",
      "-1.4186787173103763 specie\n",
      "-1.9852600019841322 habitat\n",
      "-2.1003683319600253 forest\n",
      "-2.104593247229218 decline\n",
      "-2.1958310171135462 range\n",
      "-2.3276016847098173 area\n",
      "-2.4262048282223443 individual\n",
      "-2.5189710299847814 estimate\n",
      "-2.546616190650928 species\n",
      "-2.6111640554434565 breeding\n",
      "-2.6780874383367177 occur\n",
      "-2.7433082040854355 small\n",
      "-2.7699349500251564 size\n",
      "-2.772391091610782 bird\n",
      "-2.905395660094575 record\n",
      "-2.920354499773298 year\n",
      "-2.956857930487935 island\n",
      "-2.9754065914768866 find\n",
      "-2.989056462167289 nest\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "log_nsents = log(len(words))\n",
    "bli_loglik = dict()\n",
    "\n",
    "# show top 20 \n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    tok = bli_dictionary[x[0]]\n",
    "    bli_loglik[tok] = log(x[1]) - log_nsents\n",
    "    if count > 0:\n",
    "        print(f'{bli_loglik[tok]:5} {bli_dictionary[x[0]]}')\n",
    "        count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e28680",
   "metadata": {},
   "source": [
    "This dict is now the model which we'll use for LitScan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d589689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk \n",
    "\n",
    "import json\n",
    "\n",
    "outfile = datapath + \"bli_model_11107.json\"\n",
    "with open(outfile, 'w') as jf:\n",
    "    json.dump(bli_loglik, jf)\n",
    "jf.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
