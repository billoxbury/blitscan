{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31826e27",
   "metadata": {},
   "source": [
    "# A bag-of-words topic model for BirdLife"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6f3ba",
   "metadata": {},
   "source": [
    "## Extracting sentences from the BirdLife text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85142aac",
   "metadata": {},
   "source": [
    "Load the text data scraped from BirdLife assessments. The cleaned text that we will use is in the field 'text_short'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5d4b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name_com</th>\n",
       "      <th>name_sci</th>\n",
       "      <th>SISRecID</th>\n",
       "      <th>date</th>\n",
       "      <th>text_main</th>\n",
       "      <th>text_short</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Cream-browed White-eye</td>\n",
       "      <td>Heleia superciliaris</td>\n",
       "      <td>22714307</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nAlthoug...</td>\n",
       "      <td>Although this species may have a restricted ra...</td>\n",
       "      <td>10.250856</td>\n",
       "      <td>-0.3714</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Striped Sparrow</td>\n",
       "      <td>Oriturus superciliosus</td>\n",
       "      <td>22721301</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has a very large range, and hence...</td>\n",
       "      <td>14.209733</td>\n",
       "      <td>-2.746421</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>White-chinned Prinia</td>\n",
       "      <td>Schistolais leucopogon</td>\n",
       "      <td>22713643</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>4.975323</td>\n",
       "      <td>4.849581</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Masked Water-tyrant</td>\n",
       "      <td>Fluvicola nengeta</td>\n",
       "      <td>22700284</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species has an extremely large range, and...</td>\n",
       "      <td>6.9901</td>\n",
       "      <td>-0.280215</td>\n",
       "      <td>LC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://datazone.birdlife.org/species/factsheet...</td>\n",
       "      <td>Lendu Crombec</td>\n",
       "      <td>Sylvietta chapini</td>\n",
       "      <td>22715107</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>\\n Justification of Red List Category\\nThis sp...</td>\n",
       "      <td>This species is listed as Critically Endangere...</td>\n",
       "      <td>5.344564</td>\n",
       "      <td>4.205028</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link                name_com  \\\n",
       "0  http://datazone.birdlife.org/species/factsheet...  Cream-browed White-eye   \n",
       "1  http://datazone.birdlife.org/species/factsheet...         Striped Sparrow   \n",
       "2  http://datazone.birdlife.org/species/factsheet...    White-chinned Prinia   \n",
       "3  http://datazone.birdlife.org/species/factsheet...     Masked Water-tyrant   \n",
       "4  http://datazone.birdlife.org/species/factsheet...           Lendu Crombec   \n",
       "\n",
       "                 name_sci  SISRecID        date  \\\n",
       "0    Heleia superciliaris  22714307  2022-01-31   \n",
       "1  Oriturus superciliosus  22721301  2022-01-31   \n",
       "2  Schistolais leucopogon  22713643  2022-01-31   \n",
       "3       Fluvicola nengeta  22700284  2022-01-31   \n",
       "4       Sylvietta chapini  22715107  2022-01-31   \n",
       "\n",
       "                                           text_main  \\\n",
       "0  \\n Justification of Red List Category\\nAlthoug...   \n",
       "1  \\n Justification of Red List Category\\nThis sp...   \n",
       "2  \\n Justification of Red List Category\\nThis sp...   \n",
       "3  \\n Justification of Red List Category\\nThis sp...   \n",
       "4  \\n Justification of Red List Category\\nThis sp...   \n",
       "\n",
       "                                          text_short          x         y  \\\n",
       "0  Although this species may have a restricted ra...  10.250856   -0.3714   \n",
       "1  This species has a very large range, and hence...  14.209733 -2.746421   \n",
       "2  This species has an extremely large range, and...   4.975323  4.849581   \n",
       "3  This species has an extremely large range, and...     6.9901 -0.280215   \n",
       "4  This species is listed as Critically Endangere...   5.344564  4.205028   \n",
       "\n",
       "  status  \n",
       "0     LC  \n",
       "1     LC  \n",
       "2     LC  \n",
       "3     LC  \n",
       "4     CR  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datapath = \"../../data/\"\n",
    "bli_master = datapath + \"master-BLI.csv\"\n",
    "\n",
    "df = pd.read_csv(bli_master, index_col = None).fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f286a36",
   "metadata": {},
   "source": [
    "Using <a href=\"https://spacy.io/\">spaCy</a>, we'll extract a list of distinct sentence from across all the 'text_short' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81689ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 6 --> 6\n",
      "500: 5876 --> 3772\n",
      "1000: 12448 --> 8263\n",
      "1500: 19036 --> 12734\n",
      "2000: 25598 --> 17116\n",
      "2500: 31899 --> 21098\n",
      "3000: 38509 --> 25490\n",
      "3500: 44571 --> 29263\n",
      "4000: 50407 --> 32734\n",
      "4500: 56887 --> 36945\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher                                                                                                                                                                                         \n",
    "\n",
    "# load a language model\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "#Â recognise verbs\n",
    "def verbs(sent):\n",
    "    pattern=[\n",
    "        {'POS': 'VERB', 'OP': '?'},\n",
    "        {'POS': 'ADV', 'OP': '*'},\n",
    "        {'POS': 'VERB', 'OP': '+'}\n",
    "    ]\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    # add pattern to matcher\n",
    "    matcher.add('verb-phrases', [pattern])\n",
    "    d = nlp(sent.text)\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(d)\n",
    "    spans = [d[start:end] for _, start, end in matches] \n",
    "    return spans\n",
    "\n",
    "# recognise clean sentences\n",
    "def clean_sentences(sents):\n",
    "    sentences = [s for s in sents if len(verbs(s)) > 0 and\n",
    "                    len(s) > 3]\n",
    "    return sentences\n",
    "\n",
    "# build our list, called 'sentences'\n",
    "texts = list(df['text_short'])\n",
    "sentences = []\n",
    "count = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    txt = df.at[i, 'text_short']\n",
    "    doc = nlp(txt)\n",
    "    new_sents = clean_sentences(list(doc.sents))\n",
    "    count += len(new_sents)\n",
    "    sentences += [str(x) for x in new_sents]\n",
    "    if i % 500 == 0: \n",
    "        # dedupe and show progress\n",
    "        sentences = list(set(sentences))\n",
    "        print(f'{i}: {count} --> {len(sentences)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fb3b9",
   "metadata": {},
   "source": [
    "That took a while, so let's write the sentences to disk for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c7d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = datapath + \"bli_sentences.txt\"\n",
    "with open(outfile, 'w') as fp:\n",
    "    for s in sentences:\n",
    "        fp.write(s + '\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb40c9",
   "metadata": {},
   "source": [
    "## Building a topic from the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2634a0",
   "metadata": {},
   "source": [
    "Load the sentences. (If cloning from the GitHub repo, you can start here as the sentece file is included in the <i>data</i> directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fd93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../../data/\"\n",
    "\n",
    "sentfile = datapath + \"bli_sentences.txt\"\n",
    "with open(sentfile, 'r') as fp:\n",
    "    sentences = [s.strip() for s in fp.readlines()]\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0910487",
   "metadata": {},
   "source": [
    "Load spaCy and build a list 'words'. Each entry is a list of normalised words (tokens) from a sentence in our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fd3fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20185 words in 40810 distinct sentences.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "# tags we want to remove from the text\n",
    "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "# build token list\n",
    "words = []\n",
    "for s in nlp.pipe(sentences):\n",
    "    toks = [token.lemma_.lower() for token in s\n",
    "               if token.pos_ not in removal \n",
    "               and not token.is_stop \n",
    "               and token.is_alpha]\n",
    "    words.append(toks) \n",
    "\n",
    "# check number of distinct words:\n",
    "word_set = list(set(sum(words, [])))\n",
    "print(f'Found {len(word_set)} words in {len(sentences)} distinct sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce87b08",
   "metadata": {},
   "source": [
    "We'll use the 'gensim' package to build a dictionary and track frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25718c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10801 population\n",
      " 9929 specie\n",
      " 5727 habitat\n",
      " 5456 decline\n",
      " 4832 forest\n",
      " 4529 range\n",
      " 3910 area\n",
      " 3663 individual\n",
      " 3460 estimate\n",
      " 3357 species\n",
      " 3017 size\n",
      " 2996 breeding\n",
      " 2717 small\n",
      " 2678 occur\n",
      " 2610 bird\n",
      " 2429 year\n",
      " 2123 number\n",
      " 2092 island\n",
      " 2045 record\n",
      " 2025 global\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "bli_dictionary = Dictionary(words)\n",
    "#bli_dictionary.filter_extremes(no_below = 10, \n",
    "#                           no_above = 0.5, \n",
    "#                           keep_n = 5000)\n",
    "bli_vocab = bli_dictionary.token2id.keys()\n",
    "\n",
    "# show top 20 most frequent\n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count <= 0: break\n",
    "    print(f'{x[1]:5} {bli_dictionary[x[0]]}')\n",
    "    count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb1b8",
   "metadata": {},
   "source": [
    "We want to assign importance weightings to these words in a natural way.\n",
    "The attribute 'dfs' is the number of documents (i.e. BLI sentences) containing a given token. We will convert this into a log-likelihood of seeing the token in a sentence drawn from the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f7baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3292884269813783 population\n",
      "-1.4134673813662566 specie\n",
      "-1.9637353160059945 habitat\n",
      "-2.012211228876554 decline\n",
      "-2.133666688785201 forest\n",
      "-2.198425984843638 range\n",
      "-2.34538977542044 area\n",
      "-2.410644665621037 individual\n",
      "-2.467658560348081 estimate\n",
      "-2.4978794314198165 species\n",
      "-2.60466418924079 size\n",
      "-2.6116490837627406 breeding\n",
      "-2.7093988189735034 small\n",
      "-2.723856902148734 occur\n",
      "-2.749576928083113 bird\n",
      "-2.821447499397679 year\n",
      "-2.9560969666965953 number\n",
      "-2.9708066032150384 island\n",
      "-2.9935293599229498 record\n",
      "-3.0033574488592123 global\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "log_nsents = log(len(words))\n",
    "bli_loglik = dict()\n",
    "\n",
    "# show top 20 \n",
    "count = 20\n",
    "for x in sorted(bli_dictionary.dfs.items(), key=lambda x: x[1], reverse=True):\n",
    "    tok = bli_dictionary[x[0]]\n",
    "    bli_loglik[tok] = log(x[1]) - log_nsents\n",
    "    if count > 0:\n",
    "        print(f'{bli_loglik[tok]:5} {bli_dictionary[x[0]]}')\n",
    "        count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e28680",
   "metadata": {},
   "source": [
    "This dict is now the model which we'll use for LitScan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d589689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk \n",
    "\n",
    "import json\n",
    "\n",
    "outfile = datapath + \"bli_model.json\"\n",
    "with open(outfile, 'w') as jf:\n",
    "    json.dump(bli_loglik, jf)\n",
    "jf.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
